{
   // Hyperparameters related to the main training loop.
   "training" : {

      // To use the risk-seeking policy gradient, set epsilon < 1.0 and
      // baseline="R_e"
      "epsilon" : 0.05,
      "baseline" : "R_e",

      // Control variate parameters for vanilla policy gradient. If risk-seeking
      // is used, these have no effect.
      "alpha" : 0.5,
      "b_jumpstart" : false,

      // The constant optimizer used to optimized each "const" token.
      "verbose" : true,

      // Debug level
      "debug" : 2,

      // Whether to stop early if success condition is met
      "early_stopping" : true,

      // EXPERIMENTAL: Hyperparameters related to utilizing a memory buffer.
      "use_memory" : false,
      "memory_capacity" : 1e3,
      "warm_start" : null,
      "memory_threshold" : null,

      // Parameters to control what outputs to save.
      "save_positional_entropy" : false
   },
   // The State Manager defines the inputs to the Controller
   "input_embedding": {
         // Observation hyperparameters
         "observe_action" : false,
         "observe_parent" : true,
         "observe_sibling" : true,
         "observe_dangling" : false,
         "embedding" : false,
         "embedding_dim" : 128
   },
   // Hyperparameters related to the RNN distribution over objects.
   "expression_decoder" : {
      // Maximum sequence length.
      "max_length" : 20,

      // Whether to compute TensorBoard summaries.
      "summary" : false
   }
}
